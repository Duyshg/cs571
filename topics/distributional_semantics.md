# Distributional Semantics

## Topics


## Reading

* [Vector Semantics](https://web.stanford.edu/~jurafsky/slp3/6.pdf), Jurafsky and Martin, Speech and Language Processing, 2018.
* Word2Vec: [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf), 
Mikolov et al., NIPS, 2013.
* FastText: [Enriching Word Vectors with Subword Information](http://aclweb.org/anthology/Q17-1010), Bojanowski et al., TACL, 2017.

## Supplementary

* [GloVe: Global Vectors for Word Representation](https://www.aclweb.org/anthology/D14-1162), Pennington et al., EMNLP, 2014.
* [A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning](http://icml2008.cs.helsinki.fi/papers/391.pdf), Collobert and Weston, ICML, 2008.
* [A Neural Probabilistic Language Model](https://papers.nips.cc/paper/1839-a-neural-probabilistic-language-model), Bengio et al., NIPS, 2000.
* [Regularizing and Optimizing LSTM Language Models](https://openreview.net/pdf?id=SyyGPP0TZ), Merity et al., ICLR, 2018.
* [Deep Contextualized Word Representations](https://aclweb.org/anthology/N18-1202), Peters et al., NAACL, 2018.
* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805), Devlin et al., arXiv, 2018.
